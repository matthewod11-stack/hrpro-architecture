# Digital Article / Cognitive Bias

Research: Executives Who Used Gen AI Made Worse Predictions An experiment with nearly 300 managers and executives found significant discrepancies in forecasts produced by groups who consulted with peers and those who used ChatGPT. by José Parra-Moyano, Patrick Reinmoeller, and Karl Schmedders

## Published on HBR.org / July 1, 2025 / Reprint H08SQS

# Ryan McVay/Muhla1/Getty Images

Many organizations are prioritizing the integration of AI tools into the

workplace. And for good reason—early studies have shown that they

can boost employee performance on simple or rote tasks, help leaders

become better communicators, and aid organizations in expanding

Copyright © 2025 Harvard Business School Publishing. All rights reserved.

1

This document is authorized for use only by Matt O'Donnell (matthew.od11@gmail.com). Copying or posting is an infringement of copyright. Please contact customerservice@harvardbusiness.org or 800-988-0886 for additional copies.

# HBR / Digital Article / Research: Executives Who Used Gen AI Made Worse Predictions

their customer bases. But how does AI fare as a partner in higher-stakes

# decision-making?

To test this, we ran a simple experiment. We asked more than 300

managers and executives to make stock predictions after reviewing past

trends. Half were then given the chance to confer with their peers,

while the other half could consult with ChatGPT. Participants could

then revise their predictions.

The results were striking. We found that ChatGPT made executives

## signiﬁcantly more optimistic in their forecasts while peer discussions

tended to encourage caution. Additionally, we found that the executives

armed with ChatGPT made worse predictions, based on actual stock

ﬁgures, than they had before they consulted the tool. In this article we

share what we did, what we found, what explains it, and why it matters

## for any leader integrating AI into their decision-making process.

# The Experiment

Our study took place during a series of executive-education sessions

on AI between June 2024 and March 2025. Participants were managers

and executives from various companies taking part in a class exercise.

We began by showing everyone the recent stock price chart of Nvidia

(NVDA), the leading chipmaker whose fortunes have skyrocketed

thanks to its role in powering AI technologies. Nvidia’s share price had

been on a notable rise, which made it a ﬁtting, real-world case to test

forecasting ability. (And it certainly had the executives’ attention).

First, each participant was asked to make an individual forecast: What

did they expect Nvidia’s stock price to be one month into the future?

They submitted this initial estimate privately. Next, we randomly split

## the participants into two groups for a brief consultation period:

Copyright © 2025 Harvard Business School Publishing. All rights reserved.

2

This document is authorized for use only by Matt O'Donnell (matthew.od11@gmail.com). Copying or posting is an infringement of copyright. Please contact customerservice@harvardbusiness.org or 800-988-0886 for additional copies.

# HBR / Digital Article / Research: Executives Who Used Gen AI Made Worse Predictions

- Peer discussion (control group): These executives discussed their

forecasts in small groups for a few minutes. They did not use any AI

tools, just human conversation, sharing thoughts or information they

had. This mimics a traditional approach to decision-making: getting

# input from colleagues.

- ChatGPT consultation (treatment group): These executives could ask

ChatGPT anything they liked about Nvidia’s stock (for example, to

analyze recent trends or give a one-month forecast) but they were

told not to talk to any peers. This scenario represents an AI-assisted

decision process, where an executive might consult an AI advisor

# instead of colleagues.

After this, everyone made a revised forecast for Nvidia’s price one

month ahead, and we collected those responses.

# The Results

AI led to more optimistic forecasts.

The groups started with comparable baseline expectations: Before any

discussion or AI consultation, the two sets of executives had statistically

# indistinguishable forecasts for Nvidia.

After the consultation period, however, we found that executives

who used ChatGPT became more optimistic in their forecasts. On

average, the ChatGPT group raised their one-month price estimates by

about $5.11.

Peer discussion made forecasts more conservative.

In contrast to the AI group, the peer-discussion group lowered their

price estimates, on average, by about $2.20. Additionally, this group was

more likely than the ChatGPT group to stick with their original estimate.

If they did make a change, they were more likely to decrease their initial

# forecast.

Copyright © 2025 Harvard Business School Publishing. All rights reserved.

3

This document is authorized for use only by Matt O'Donnell (matthew.od11@gmail.com). Copying or posting is an infringement of copyright. Please contact customerservice@harvardbusiness.org or 800-988-0886 for additional copies.

# HBR / Digital Article / Research: Executives Who Used Gen AI Made Worse Predictions

These patterns held true even when we controlled for the most extreme

(high or low) forecasts in our dataset.

AI consultation made predictions worse.

After we conducted our study, we waited a month, and then compared

the predictions the executives had made against actual Nvidia

data. We found that both groups were too optimistic, on average.

However, we found that those who used ChatGPT made even worse

predictions after their consultation. Those who discussed with their

peers made signiﬁcantly better predictions than they’d made before

# their consultation.

AI caused overconﬁdence.

## Among all of our participants, about a third oﬀered pin-point

predictions in their initial estimates (meaning they oﬀered ﬁgures with

one or more decimals) which previous research has established as an

indicator of overconﬁdence. What surprised us was that after consulting

with peers or ChatGPT, participants’ overconﬁdence in the accuracy of

their forecasts systematically changed.

Conversing with ChatGPT signiﬁcantly increased the participants’

tendency to oﬀer pin-point predictions, while participants in the peer

discussion group became signiﬁcantly less likely to use pinpoints in

their revised estimate. In other words, their overconﬁdence decreased.

## Why AI Created Overconfidence and Optimism

What might explain these very diﬀerent outcomes? Why would

consulting an AI tool lead to inﬂated estimates and overconﬁdence,

while peer discussions provoked greater humility and conservatism? We

have isolated ﬁve reasons:

Copyright © 2025 Harvard Business School Publishing. All rights reserved.

4

This document is authorized for use only by Matt O'Donnell (matthew.od11@gmail.com). Copying or posting is an infringement of copyright. Please contact customerservice@harvardbusiness.org or 800-988-0886 for additional copies.

# HBR / Digital Article / Research: Executives Who Used Gen AI Made Worse Predictions

- 1. Extrapolation and “trend riding”

ChatGPT may have encouraged extrapolation bias. The AI’s knowledge

is based on historical data, so it might have simply extended Nvidia’s

recent upward trend into the future. Indeed, Nvidia’s stock had been

climbing steeply in the months leading up to our sessions. Lacking

up-to-the-minute context or any sense of an upcoming turning point,

ChatGPT’s analysis likely assumed “what has been going up will keep

going up.” The AI’s guidance, based purely on past data patterns, may

have skewed toward optimism by default.

- 2. Authority bias and detail overload

Many executives in the ChatGPT condition reported being impressed

## by the detail and conﬁdent tone of the AI’s answer. In post-experiment

discussions, some participants noted that ChatGPT provided a wealth of

data and reasoning that was so thorough and self-assured, it made their

own initial estimates seem inadequate by comparison.

This is a form of AI authority bias: Because the AI spoke in a conﬁdent,

analytical manner, users tended to give its suggestions a lot of weight,

sometimes more than they gave their own judgment. Essentially, the

medium’s authority (an advanced AI, sounding like an expert report)

boosted the credibility of an optimistic forecast.

- 3. Emotion (or lack thereof)

Humans have emotions and instincts that can act as a check on extreme

forecasts. An executive looking at a meteoric stock chart might feel a

tinge of wariness and an inner voice saying “If it’s at a peak, it could

crash.” In our sessions, we suspect that emotional caution played a role

in the peer discussions. People might voice doubts or fears (“This stock

feels bubbly; maybe we should rein it in a bit”).

ChatGPT, on the other hand, has no such emotion or intuition. It doesn’t

feel fear of heights the way a person might when seeing a price chart

Copyright © 2025 Harvard Business School Publishing. All rights reserved.

5

This document is authorized for use only by Matt O'Donnell (matthew.od11@gmail.com). Copying or posting is an infringement of copyright. Please contact customerservice@harvardbusiness.org or 800-988-0886 for additional copies.

# HBR / Digital Article / Research: Executives Who Used Gen AI Made Worse Predictions

soaring. The AI oﬀers an analysis uncolored by anxiety, which can

be useful, but also means it might not second guess an optimistic

trend. Users who rely on the AI’s output alone don’t get the beneﬁt

of that cautious gut-check. In our experiment, the absence of “fear of

being wrong” on the AI side may have allowed bolder forecasts to go

# untampered.

- 4. Peer calibration and social dynamics

The act of discussing with peers introduced a diﬀerent set of biases and

behaviors, generally pushing toward caution and consensus. In a group

discussion, individuals hear diverse viewpoints and often discover that

others’ expectations diﬀer from their own. In our peer groups, this

dynamic often led to moderating extreme views and ﬁnding a middle

# ground.

Additionally, in professional settings, no one wants to be the person

with an absurdly bullish forecast. There is a bit of “don’t be the sucker”

mentality, because executives know that unbridled optimism can look

naïve. In our view, this may have created a spiral of skepticism in the

group setting: each person, consciously or not, trying not to appear too

rosy-eyed, resulted in collectively lower forecasts. This is almost the

opposite of classic “groupthink”—instead of cheerleading each other

into euphoria, these executives reined each other in, perhaps to avoid

standing out. The end result was a more conservative consensus.

- 5. Illusion of knowledge

Research has shown that when people have access to vast bodies of

## information, like the internet, or tools for information processing, like

computers, they are more likely to fall for the illusion of knowing it

all. A signiﬁcantly large number of participants show the eﬀect of this

illusion when they tapped into ChatGPT, one of the most intelligent

## technologies with access to vast resources.

Copyright © 2025 Harvard Business School Publishing. All rights reserved.

6

This document is authorized for use only by Matt O'Donnell (matthew.od11@gmail.com). Copying or posting is an infringement of copyright. Please contact customerservice@harvardbusiness.org or 800-988-0886 for additional copies.

# HBR / Digital Article / Research: Executives Who Used Gen AI Made Worse Predictions

## How to Use AI Wisely in Executive Decisions

Our ﬁndings carry some important lessons for leaders and

organizations as they integrate AI tools into decision-making:

- 1. Be aware of AI’s biases and the illusion of knowledge.

Consulting ChatGPT (or similar gen AI models) can subtly but

signiﬁcantly tilt your expectations and create excessive conﬁdence in

them. If you’re using AI to inform forecasts or strategic decisions,

remember that it might impress you with detail while lacking the

cautionary perspective that humans naturally bring. Hence, when

making predictions, specify the data you want the system to analyze,

have the system provide you with a statistical conﬁdence interval, and

most importantly (to compensate any bias), ask the system to explain

why or how the prediction could be wrong.

- 2. Don’t ditch human discussion, instead leverage it.

The contrast between our two groups underscores the value of peer

discussion as a counterbalance. Yes, AI can provide lightning-fast

analysis and information, but there is still wisdom in the room. Peers

can contribute fresh data, context, and a reality check on ideas that

sound too good to be true, or just too precise in their forecast. In

practice, the best approach may be to combine AI input with human

dialogue. For instance, an executive could get a quick take from

ChatGPT and then convene a team meeting to debate it. Our research

suggests that this combination would harness AI’s strengths and

human judgment, helping avoid the blind spots of either alone. As

leaders, encouraging a culture where new tech is used alongside healthy

skepticism and discussion will likely lead to more balanced decisions.

- 3. Critical thinking is key, no matter the source.

Whether advice comes from an AI or a colleague, executives must apply

their own critical thinking. Ask questions about the basis of a forecast.

In the case of ChatGPT, one might probe: What data is this forecast

Copyright © 2025 Harvard Business School Publishing. All rights reserved.

7

This document is authorized for use only by Matt O'Donnell (matthew.od11@gmail.com). Copying or posting is an infringement of copyright. Please contact customerservice@harvardbusiness.org or 800-988-0886 for additional copies.

# HBR / Digital Article / Research: Executives Who Used Gen AI Made Worse Predictions

drawing on? Could there be recent factors it is missing? With peers, one

might ask: Are we being too conservative because we’re all uncertain?

Especially with AI, which can sound authoritative, it is crucial not to

accept recommendations at face value. Treat AI as a starting point for

# inquiry, not the ﬁnal word.

This also applies to our very own study. This study was conducted in a controlled executive‐education setting with a limited sample of managers and may not fully capture the complexity or emotional stakes of real‐world trading environments. We also focused exclusively on a single stock (Nvidia) and a one-month forecasting horizon, so results may diﬀer for other securities or longer‐term predictions. Finally, the ChatGPT model we used lacked access to up-to-the-minute market data

and alternative AI tools or more current model versions might yield

diﬀerent eﬀects. Understanding limitations or possible confounders

(both in our study) and with AI can help you apply a critical lens to

# outputs.

- 4. Train and set guidelines for AI use in teams.

If your organization is rolling out AI advisors or assistants, make it

known that AI might encourage overconﬁdence. Like Ulysses needed

wax and string to withstand the lure of the Sirens, your organization

needs guidelines on how to handle AI. For example, before getting an

AI-generated forecast, require a round of peer discussion or a review of

worst-case scenarios before consulting with AI. By institutionalizing a

mix of AI and human input, you guard against lopsided inﬂuences.

. . .

AI is changing how decisions are made, but human judgment remains

crucial. Our experiment provides a data-backed glimpse into this

evolving dynamic. Even seasoned executives can be swayed by a

convincing AI argument, and even enthusiastic groups can become

cautious with a bit of discussion. Neither approach is “better” in all

Copyright © 2025 Harvard Business School Publishing. All rights reserved.

8

This document is authorized for use only by Matt O'Donnell (matthew.od11@gmail.com). Copying or posting is an infringement of copyright. Please contact customerservice@harvardbusiness.org or 800-988-0886 for additional copies.

# HBR / Digital Article / Research: Executives Who Used Gen AI Made Worse Predictions

cases. Sometimes optimism is warranted, and sometimes caution is.

The takeaway for leaders is to be mindful of where the advice is coming

from. If it is from AI, inject some human common sense; if it is from

humans, you might even use AI to ensure you did not miss an outside

# perspective.

This article was originally published online on July 1, 2025.

José Parra-Moyano is a Professor of Digital Strategy at IMD Business School. His research focuses on how organizations can use digital technologies to create value. He is a passionate and award-winning teacher who founded his own startup, who has been listed as a Thinkers50 Radar List member, as a Forbes 30 under 30, and as a Global Shaper by the World Economic Forum.

Patrick Reinmoeller is Professor of Strategy and Innovation at IMD, Switzerland, where he helps leaders transform their multinational organizations. He is a co-founder and non-executive board member. Before IMD, he taught at Cranﬁeld University, Rotterdam School of Management, JAIST in Japan, and LIUC in Italy and studied at Bocconi University, Hitotsubashi University, and earned a PhD at University of Cologne.

Karl Schmedders is Professor of Finance at IMD and a Visiting Professor of Executive MBA Education at the Kellogg School of Management. He is a Fellow of the Game Theory Society and of the Society for the Advancement of Economic Theory (SAET), recognized for his contributions to economic modeling and decision sciences. He earned his PhD in Operations Research from Stanford University.

Copyright © 2025 Harvard Business School Publishing. All rights reserved.

9

This document is authorized for use only by Matt O'Donnell (matthew.od11@gmail.com). Copying or posting is an infringement of copyright. Please contact customerservice@harvardbusiness.org or 800-988-0886 for additional copies.